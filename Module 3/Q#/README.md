# Maxcut QAOA - Q#

This tutorial attempts to implement the Maxcut QAOA in Q#. While we had previously implemented this through various Python frameworks, their nature as declarative commands in an interpreted language makes it difficult to generate a unified hybrid IR. We hope that the Q# compiler's QIR generation is able to do this.

## Q# Code

A QAOA instance for the [TSP](https://en.wikipedia.org/wiki/Travelling_salesman_problem) can be found [here](https://github.com/microsoft/Quantum/blob/main/samples/simulation/qaoa/QAOA.qs), with additional resources [here](http://quantumalgorithmzoo.org/traveling_santa/). This was borrowed from extensively and adapted to address our Maxcut instance.

To use the code, ensure you have set up Q# as laid out [here](../../Module%202/3_IR_Analysis/README.md#q-setup). Then, you can simply run
```terminal
dotnet run --num-trials <number of trials> --verbose <True or False>
```
from the current Q# directory. Note that both `0101` and `1010` are valid solutions to the problem instance, yielding cuts of value 4.

## QIR
The LLVM generated from this code is available [here](Maxcut_QAOA.ll). If you'd like to generate it yourself, add the property
```terminal
<QIRGeneration>true</QIRGeneration>
```

into the PropertyGroup in the [Maxcut.csproj](Maxcut.csproj) file.

## Future Work
As of now, the program is running 100 trials on the same parameters, which were generated by the optimizer utilized in the [Pennylane tutorial](../Python/Pennylane/Maxcut.ipynb). In the future, we hope to add this functionality so that this program truly has a non-trivial classical component. We see three potential paths forward:

+ Write our own simple gradient-descent optimizer from scratch in Q#
+ Leverage an existing ML library for C#, and write a wrapper/driver program for the current QAOA circuit
+ Adapt our current circuit so that we can use [Q#'s ML library](https://docs.microsoft.com/en-us/qsharp/api/qsharp/microsoft.quantum.machinelearning)

In particular, if we can use the [EstimateGradient](https://docs.microsoft.com/en-us/qsharp/api/qsharp/microsoft.quantum.machinelearning.estimategradient) function from the Q# ML library, writing our own gradient-descent optimizer should become fairly simple.

